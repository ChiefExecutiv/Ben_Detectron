{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0aos7UD+AESd5HiVXeQpb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChiefExecutiv/Ben_Detectron/blob/main/DumE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DumE** is a small language model meant for study and experimentation.\n",
        "This model is my naive attempt at creating a language model AI.\n",
        "It is trained on a wikipidea article \"The origins of the universe\", focusing on the Big Bang Theory.\n",
        "\n",
        "You will notice the output below, it's not very coherent but it's not too bad either. I will emphasize that this model is not built for performance.\n",
        "Reproducing a reasonable language model would require multiple training hours in a very powerful compute environment, I have a cheap laptop :)\n"
      ],
      "metadata": {
        "id": "lUJYOQfsCyEP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Implementation\n",
        "\n",
        "Below is the full implementation of DumE.\n",
        "A few technical details:\n",
        "It's a decoder only model hence utilizes Causal Self-attention for masking future tokens in the input sequence.\n",
        "If you're familiar with self-attention, Causal self-attention is a variant of that, it essentially restricts attention to only tensors to the left of the seqence. The model can't \"attend\" to future tokens.\n"
      ],
      "metadata": {
        "id": "jKftVTAxHDhd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LFQbABqh9wGg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import math\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CausalSelfAttention, self).__init__()\n",
        "\n",
        "        self.embed_dim = config.embed_dim\n",
        "        self.num_heads = config.num_heads\n",
        "\n",
        "        if config.embed_dim % config.num_heads != 0:\n",
        "            raise ValueError(\"Embedding dimension must be divisible by number of heads\")\n",
        "\n",
        "        # Query, Key and Value projections\n",
        "        self.qkv_proj = nn.Linear(config.embed_dim, 3 * config.embed_dim) # These can also be projected individaully\n",
        "\n",
        "        # Output Projection\n",
        "        self.output_proj = nn.Linear(config.embed_dim, config.embed_dim)\n",
        "\n",
        "        # dropout to avoid overfitting\n",
        "        self.attn_dropout = nn.Dropout(config.attn_drop)\n",
        "        self.resid_dropout = nn.Dropout(config.resid_drop)\n",
        "\n",
        "        # Returns a causal mask to ensure attention isn't applied to future tokens. It's applied to the left in the input sequence\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimension\n",
        "\n",
        "        # Query, key, values for all the heads - a head is essentially a single instance of the attention mechanism\n",
        "        qkv = self.qkv_proj(x) # Shape: (B, T, 3*C)\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1) # Split into query, key and value tensors\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        k = k.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "        v = v.view(B, T, self.num_heads, C // self.num_heads).transpose(1, 2)\n",
        "\n",
        "        # compute attention scores\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(C // self.num_heads))\n",
        "\n",
        "        # Apply causal mask\n",
        "        causal_mask = self.bias[:, :, :T, :T]  # Extract relevant part of the mask\n",
        "        attn_scores = attn_scores.masked_fill(causal_mask == 0, float('-inf'))\n",
        "\n",
        "        # Compute attention probabilities\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        # Attention output\n",
        "        attn_output = attn_probs @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)  # Reassemble heads\n",
        "\n",
        "        # Output projection\n",
        "        y = self.resid_dropout(self.output_proj(attn_output))\n",
        "        return y\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(config.embed_dim)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "        # Feed-forward neural network\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(config.embed_dim, 4 * config.embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.embed_dim, config.embed_dim),\n",
        "            nn.Dropout(config.resid_drop)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply LayerNorm, Attention, and Residual Connection\n",
        "        attn_output = self.attn(self.ln_1(x))\n",
        "        x = x + attn_output\n",
        "\n",
        "        # Apply LayerNorm, MLP, and Residual Connection\n",
        "        mlp_output = self.mlp(self.ln_2(x))\n",
        "        x = x + mlp_output\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class DumE(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(DumE, self).__init__()\n",
        "\n",
        "        # Embedding layers\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.embed_dim)\n",
        "        self.position_embedding = nn.Embedding(config.block_size, config.embed_dim)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.blocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.num_layers)])\n",
        "\n",
        "        # LayerNorm before output\n",
        "        self.ln_f = nn.LayerNorm(config.embed_dim)\n",
        "\n",
        "        # Output projection layer\n",
        "        self.head = nn.Linear(config.embed_dim, config.vocab_size, bias=False)\n",
        "\n",
        "        # Store block size\n",
        "        self.block_size = config.block_size\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.size()\n",
        "\n",
        "        # Ensure sequence length does not exceed block size\n",
        "        assert T <= self.block_size, \"Sequence length exceeds model block size\"\n",
        "\n",
        "        # Compute token and position embeddings\n",
        "        token_embeddings = self.token_embedding(idx)  # Shape: (B, T, C)\n",
        "        position_ids = torch.arange(0, T, dtype=torch.long, device=idx.device).unsqueeze(0)\n",
        "        position_embeddings = self.position_embedding(position_ids)  # Shape: (1, T, C)\n",
        "\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        # Pass through Transformer blocks\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Apply final LayerNorm and output projection\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        # Compute loss if targets are provided\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "            return logits, loss\n",
        "\n",
        "        return logits\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Crop the context window to block size\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "\n",
        "            # Forward pass to get logits\n",
        "            logits = self(idx_cond)\n",
        "\n",
        "            # Focus only on the last token's logits\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply softmax to convert logits to probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append the sampled token to the sequence\n",
        "            idx = torch.cat((idx, next_token), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Trainer\n",
        "This is responsible for creating the model's training run.\n",
        "It's an implementation of Andre Karpathy's trainer but with a few changes and a few components I stripped away for simplicity.\n"
      ],
      "metadata": {
        "id": "pCgVbHurEr1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "The Trainer class is courtesy of Andrej Karpathy\n",
        "\"\"\"\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, config, model, train_dataset):\n",
        "        self.config = config\n",
        "        self.model = model\n",
        "        self.optimizer = None\n",
        "        self.train_dataset = train_dataset\n",
        "        self.callbacks = defaultdict(list)\n",
        "\n",
        "        # The device to be trained on, a gpu is heavily recommended\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "        print(f\"Running on device: {self.device}\")\n",
        "\n",
        "\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = 0.0\n",
        "        self.iter_dt = 0.0\n",
        "\n",
        "    def add_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent].append(callback)\n",
        "\n",
        "    def set_callback(self, onevent: str, callback):\n",
        "        self.callbacks[onevent] = [callback]\n",
        "\n",
        "    def trigger_callbacks(self, onevent: str):\n",
        "        for callback in self.callbacks.get(onevent, []):\n",
        "            callback(self)\n",
        "\n",
        "    def run(self):\n",
        "        model, config = self.model, self.config\n",
        "\n",
        "        # setup the optimizer\n",
        "        self.optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "        config.grad_norm_clip = 1.0\n",
        "\n",
        "        # dataloader\n",
        "        train_loader = DataLoader(\n",
        "            self.train_dataset,\n",
        "            sampler = torch.utils.data.RandomSampler(self.train_dataset, replacement=True, num_samples=int(1e10)),\n",
        "            shuffle = False,\n",
        "            pin_memory = True,\n",
        "            batch_size = config.batch_size,\n",
        "            num_workers = config.num_workers,\n",
        "        )\n",
        "\n",
        "        model.train()\n",
        "        self.iter_num = 0\n",
        "        self.iter_time = time.time()\n",
        "        data_iter = iter(train_loader)\n",
        "        while True:\n",
        "\n",
        "            # fetch the next batch(x, y)\n",
        "            try:\n",
        "                batch = next(data_iter)\n",
        "            except StopIteration:\n",
        "                data_iter = iter(train_loader)\n",
        "                batch = next(data_iter)\n",
        "            batch = [t.to(self.device) for t in batch]\n",
        "            x, y = batch\n",
        "\n",
        "            # forward the model\n",
        "            logits, self.loss = model(x, y)\n",
        "\n",
        "            # backpropagation and update parameters\n",
        "            model.zero_grad(set_to_none=True)\n",
        "            self.loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm(model.parameters(), config.grad_norm_clip)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.trigger_callbacks('on_batch_end')\n",
        "            self.iter_num += 1\n",
        "            tnow = time.time()\n",
        "            self.iter_dt = tnow - self.iter_time\n",
        "            self.iter_time = tnow\n",
        "\n",
        "            # termination conditions\n",
        "            if config.max_iters is not None and self.iter_num >= config.max_iters:\n",
        "                break"
      ],
      "metadata": {
        "id": "cr6Udebf-hHl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "With the model implemented, a trainer in place, we're ready to train DumE.\n",
        "Note: Training on a cpu was really nerve-wrecking, which is why I moved development to this colab environment."
      ],
      "metadata": {
        "id": "ECIRzTVAGFxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Configuration for the language model.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, block_size, embed_dim, num_heads, num_layers, dropout, attn_drop, resid_drop):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.block_size = block_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout = dropout\n",
        "        self.attn_drop = attn_drop\n",
        "        self.resid_drop = resid_drop\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary of the configuration.\n",
        "        \"\"\"\n",
        "        config_dict = vars(self)\n",
        "        print(\"Model Configuration:\")\n",
        "        for key, value in config_dict.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "\n",
        "class TrainerConfig:\n",
        "    \"\"\"\n",
        "    Configuration for training.\n",
        "    \"\"\"\n",
        "    def __init__(self, batch_size, num_workers, learning_rate, max_iters, eval_interval, eval_iters):\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iters = max_iters\n",
        "        self.eval_interval = eval_interval\n",
        "        self.eval_iters = eval_iters\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary of the training configuration.\n",
        "        \"\"\"\n",
        "        config_dict = vars(self)\n",
        "        print(\"Trainer Configuration:\")\n",
        "        for key, value in config_dict.items():\n",
        "            print(f\"{key}: {value}\")\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.vocab_size = len(chars)\n",
        "        self.block_size = block_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        dix = [self.stoi[ch] for ch in chunk]\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load text data\n",
        "    with open(\"/content/sample_data/big-bang.txt\", \"r\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Initialize dataset and configurations\n",
        "    block_size = 128\n",
        "    train_dataset = CharDataset(text, block_size)\n",
        "\n",
        "    model_config = ModelConfig(\n",
        "        vocab_size=train_dataset.vocab_size,\n",
        "        block_size=block_size,\n",
        "        embed_dim=256,\n",
        "        num_heads=8,\n",
        "        num_layers=6,\n",
        "        dropout=0.1,\n",
        "        attn_drop=0.1,\n",
        "        resid_drop=0.1\n",
        "    )\n",
        "\n",
        "    trainer_config = TrainerConfig(\n",
        "        batch_size=32,\n",
        "        num_workers=2,\n",
        "        learning_rate=3e-4,\n",
        "        max_iters=3000,\n",
        "        eval_interval=100,\n",
        "        eval_iters=10\n",
        "    )\n",
        "\n",
        "    # Create model and trainer\n",
        "    model = DumE(model_config)\n",
        "    trainer = Trainer(trainer_config, model, train_dataset)\n",
        "\n",
        "    # Print configurations\n",
        "    model_config.summary()\n",
        "    trainer_config.summary()\n",
        "\n",
        "    # Train the model\n",
        "    trainer.run()\n",
        "\n",
        "    # Save the model\n",
        "    torch.save(model.state_dict(), \"trained_model.pt\")\n",
        "    print(\"Model saved as trained_model.pt\")\n",
        "\n",
        "    # Generate samples\n",
        "    with torch.no_grad():\n",
        "        context = \"The Big Bang\"\n",
        "        x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None, ...].to(trainer.device)\n",
        "        y = model.generate(x, 500)[0]\n",
        "        completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "        print(\"Generated text:\")\n",
        "        print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lvc5LLw_-7tU",
        "outputId": "302fb980-8ece-4eb8-e9f0-f060e43d2658"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on device: cuda\n",
            "Model Configuration:\n",
            "vocab_size: 87\n",
            "block_size: 128\n",
            "embed_dim: 256\n",
            "num_heads: 8\n",
            "num_layers: 6\n",
            "dropout: 0.1\n",
            "attn_drop: 0.1\n",
            "resid_drop: 0.1\n",
            "Trainer Configuration:\n",
            "batch_size: 32\n",
            "num_workers: 2\n",
            "learning_rate: 0.0003\n",
            "max_iters: 3000\n",
            "eval_interval: 100\n",
            "eval_iters: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-eee791300b38>:81: FutureWarning: `torch.nn.utils.clip_grad_norm` is now deprecated in favor of `torch.nn.utils.clip_grad_norm_`.\n",
            "  torch.nn.utils.clip_grad_norm(model.parameters(), config.grad_norm_clip)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as trained_model.pt\n",
            "Generated text:\n",
            "The Big Bang was preceded by Sthew the eBig Bang theory's confluction is a comprehensive ever must have been withound \n",
            "and out 20106 – each last horizon as muched that the universe, and today is very emery nearly possible trans; and the oldn baryon matter; and through galaxies ward and quasars and galaxies (with as billion years.\n",
            "\n",
            "The universe resumpirality\n",
            "In a vistance of the age of the universe\n",
            "If the earliest anst describes then any ampecularing atums \n",
            "he uniforpred every fred the expansion of the unive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above is model output from the prompt \"The Big bang\" not too bad given that it trained for about 2 minutes on remote gpu."
      ],
      "metadata": {
        "id": "dWOgNad0IOL-"
      }
    }
  ]
}